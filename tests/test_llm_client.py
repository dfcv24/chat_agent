#!/usr/bin/env python3
"""
LLMå®¢æˆ·ç«¯æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–°çš„å¤§æ¨¡å‹å®¢æˆ·ç«¯åŠŸèƒ½å’Œä¿¡æ¯æå–èƒ½åŠ›
"""

import os
import sys

# æ·»åŠ é¡¹ç›®ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_llm_client():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯åŸºç¡€åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•LLMå®¢æˆ·ç«¯...")
    
    try:
        from llm_client import get_llm_client, LLMClient
        from config import ChatConfig
        
        # æµ‹è¯•é…ç½®åŠ è½½
        config = ChatConfig()
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   - APIå¯ç”¨: {'æ˜¯' if config.API_KEY else 'å¦'}")
        print(f"   - æ¨¡å‹: {config.CHAT_MODEL_NAME}")
        print(f"   - APIåœ°å€: {config.API_BASE_URL}")
        
        # æµ‹è¯•LLMå®¢æˆ·ç«¯åˆå§‹åŒ–
        llm_client = get_llm_client()
        print(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - å¯ç”¨çŠ¶æ€: {'æ˜¯' if llm_client.is_available else 'å¦'}")
        
        if not llm_client.is_available:
            print("âš ï¸  LLMä¸å¯ç”¨ï¼Œå°†æµ‹è¯•fallbackåŠŸèƒ½")
        
        # æµ‹è¯•ç®€å•èŠå¤©åŠŸèƒ½
        print("\nğŸ”§ æµ‹è¯•ç®€å•èŠå¤©åŠŸèƒ½...")
        if llm_client.is_available:
            response = llm_client.simple_chat(
                "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±",
                system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹"
            )
            if response:
                print(f"âœ… èŠå¤©æµ‹è¯•æˆåŠŸ: {response[:100]}...")
            else:
                print("âŒ èŠå¤©æµ‹è¯•å¤±è´¥")
        else:
            print("âš ï¸  è·³è¿‡èŠå¤©æµ‹è¯•ï¼ˆLLMä¸å¯ç”¨ï¼‰")
        
        # æµ‹è¯•JSONæå–åŠŸèƒ½
        print("\nğŸ”§ æµ‹è¯•JSONä¿¡æ¯æå–...")
        test_cases = [
            {
                "input": "æˆ‘å«å¼ ä¸‰ï¼Œä»Šå¹´25å²",
                "prompt": "ä»ç”¨æˆ·è¾“å…¥ä¸­æå–å§“åå’Œå¹´é¾„ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š{\"name\": \"å§“å\", \"age\": å¹´é¾„}"
            },
            {
                "input": "ä¸æƒ³è¯´",
                "prompt": "ä»ç”¨æˆ·è¾“å…¥ä¸­æå–ä¸ªäººä¿¡æ¯ï¼Œå¦‚æœç”¨æˆ·æ‹’ç»åˆ™è¿”å›ç©ºJSONå¯¹è±¡"
            }
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"   æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['input']}")
            result = llm_client.extract_json(
                test_case["input"],
                test_case["prompt"],
                fallback_value={}
            )
            print(f"   ç»“æœ: {result}")
        
        print("\nğŸ‰ LLMå®¢æˆ·ç«¯æµ‹è¯•å®Œæˆï¼")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²å®‰è£…")
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

def test_knowledge_extraction():
    """æµ‹è¯•çŸ¥è¯†æå–åŠŸèƒ½"""
    print("\nğŸ§ª å¼€å§‹æµ‹è¯•çŸ¥è¯†æå–åŠŸèƒ½...")
    
    try:
        from knowledge_manager import KnowledgeManager
        
        # åˆ›å»ºçŸ¥è¯†ç®¡ç†å™¨
        km = KnowledgeManager()
        print("âœ… çŸ¥è¯†ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ä¿¡æ¯æå–
        test_cases = [
            ("æˆ‘å«æå››ï¼Œä»Šå¹´30å²ï¼Œæ˜¯ä¸ªç¨‹åºå‘˜", "è¯·å‘Šè¯‰æˆ‘ä½ çš„åŸºæœ¬ä¿¡æ¯"),
            ("æˆ‘å–œæ¬¢çœ‹ç”µå½±å’Œè¯»ä¹¦", "ä½ æœ‰ä»€ä¹ˆçˆ±å¥½ï¼Ÿ"),
            ("ä¸æƒ³è¯´", "ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ"),
            ("æˆ‘ä½åœ¨åŒ—äº¬", "ä½ åœ¨å“ªä¸ªåŸå¸‚ï¼Ÿ"),
            ("175cm", "ä½ çš„èº«é«˜æ˜¯å¤šå°‘ï¼Ÿ")
        ]
        
        for user_response, question in test_cases:
            print(f"\n   é—®é¢˜: {question}")
            print(f"   å›ç­”: {user_response}")
            
            extracted = km.extract_info_from_response(user_response, question)
            print(f"   æå–ç»“æœ: {extracted}")
            
            if extracted:
                updated = km.update_knowledge(extracted)
                print(f"   æ›´æ–°çŠ¶æ€: {'æˆåŠŸ' if updated else 'æ— éœ€æ›´æ–°'}")
        
        print("\nğŸ‰ çŸ¥è¯†æå–æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ çŸ¥è¯†æå–æµ‹è¯•å¤±è´¥: {e}")

def test_chatbot_integration():
    """æµ‹è¯•èŠå¤©æœºå™¨äººé›†æˆ"""
    print("\nğŸ§ª å¼€å§‹æµ‹è¯•èŠå¤©æœºå™¨äººé›†æˆ...")
    
    try:
        from chatbot import ChatBot
        
        # åˆ›å»ºèŠå¤©æœºå™¨äºº
        bot = ChatBot()
        print("âœ… èŠå¤©æœºå™¨äººåˆå§‹åŒ–æˆåŠŸ")
        print(f"   - LLMå¯ç”¨: {'æ˜¯' if bot.llm_client.is_available else 'å¦'}")
        print(f"   - çŸ¥è¯†ç®¡ç†: {'å¯ç”¨' if bot.config.ENABLE_KNOWLEDGE_LEARNING else 'ç¦ç”¨'}")
        print(f"   - ä¸»åŠ¨æé—®: {'å¯ç”¨' if bot.config.AUTO_ASK_QUESTIONS else 'ç¦ç”¨'}")
        
        # æµ‹è¯•åŸºç¡€å›å¤åŠŸèƒ½
        if bot.llm_client.is_available:
            print("\n   æµ‹è¯•åŸºç¡€å¯¹è¯...")
            test_response = bot.get_response("ä½ å¥½")
            if test_response and not test_response.startswith("âŒ"):
                print(f"âœ… å¯¹è¯æµ‹è¯•æˆåŠŸ: {test_response[:50]}...")
            else:
                print(f"âŒ å¯¹è¯æµ‹è¯•å¤±è´¥: {test_response}")
        else:
            print("âš ï¸  è·³è¿‡å¯¹è¯æµ‹è¯•ï¼ˆLLMä¸å¯ç”¨ï¼‰")
        
        print("\nğŸ‰ èŠå¤©æœºå™¨äººé›†æˆæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ èŠå¤©æœºå™¨äººæµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ å¼€å§‹LLMå®¢æˆ·ç«¯å’Œä¿¡æ¯æå–åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    test_llm_client()
    test_knowledge_extraction()
    test_chatbot_integration()
    
    print("\n" + "=" * 60)
    print("âœ¨ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. æ–°çš„LLMå®¢æˆ·ç«¯æ”¯æŒå¤šç§è°ƒç”¨æ–¹å¼")
    print("2. ä¿¡æ¯æå–ç°åœ¨ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ„å›¾è¯†åˆ«")
    print("3. åœ¨ä»»ä½•æ–‡ä»¶ä¸­éƒ½å¯ä»¥é€šè¿‡ get_llm_client() è·å–LLMå®ä¾‹")
    print("4. è‡ªåŠ¨fallbackåˆ°è§„åˆ™åŒ¹é…ï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰")
